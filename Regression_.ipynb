{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1.What is Simple Linear Regression?\n",
        "  - Simple Linear Regression is a statistical method used to model the     relationship between two variables:\n",
        "\n",
        "   - One independent variable (also called predictor or input, denoted as X)\n",
        "\n",
        "   - One dependent variable (also called response or output, denoted as Y)\n",
        "\n",
        "   e.g:Predicting selling price of a bike (Y) based on how much it's been driven (X = km_driven).\n",
        "\n",
        "#2.What are the key assumptions of Simple Linear Regression?\n",
        "  - Linearity:The relationship between the independent variable (X) and the   dependent variable (Y) is linear.\n",
        "  - Independence of Errors:Observations should be independent of each other.\n",
        "  - Homoscedasticity (Constant Variance of Errors):The residuals (errors)  should have constant variance at all levels of X.\n",
        "  - Normality of Errors:The residuals (differences between actual and predicted Y) should be normally distributed.\n",
        "  - No Multicollinearity\n",
        "\n",
        "#3.What does the coefficient m represent in the equation Y=mX+c?\n",
        "  - The coefficient m represents: The slope of the line ‚Äî or the rate of change of Y with respect to X.\n",
        "   In machine learning:\n",
        "   - m is the learned weight (or coefficient) for the feature X in simple  linear regression.\n",
        "\n",
        "   - It‚Äôs estimated using the data to minimize the difference between predicted and actual Y.\n",
        "\n",
        "#4.What does the intercept c represent in the equation Y=mX+c?\n",
        "  - The intercept c represents:\n",
        "   The value of Y when X = 0 ‚Äî it's the point where the line crosses the Y-axis.\n",
        "  - In machine learning:The intercept is learned from the data during training, just like the slope m.\n",
        "\n",
        "  - It helps adjust the line to best fit the data even if Y doesn't start at 0.\n",
        "\n",
        "\n",
        "#5.How do we calculate the slope m in Simple Linear Regression?\n",
        "  - The slope m in simple linear regression represents the change in the dependent variable (Y) for every one-unit change in the independent variable (X).\n",
        "   Formula: m = Cov(x,y)/Var(X)\n",
        "   Example: If m = 2, then for every 1 unit increase in X, Y increases by 2 units.\n",
        "\n",
        "#6.What is the purpose of the least squares method in Simple Linear Regression?\n",
        "  - The purpose of the least squares method in Simple Linear Regression is to   find the best-fitting straight line (also called the regression line) through a set of data points. This line is defined in the form:\n",
        "                 y= Œ≤0 + Œ≤1x\n",
        "\n",
        "     Where:y is the dependent variable (response),\n",
        "\n",
        "      x is the independent variable (predictor),\n",
        "\n",
        "       ùõΩ0 is the intercept,\n",
        "\n",
        "       ùõΩ1 is the slope.\n",
        "\n",
        "\n",
        "#7.How is the coefficient of determination (R¬≤) interpreted in Simple Linear Regression?\n",
        "  - The coefficient of determination, denoted as R¬≤ , is a key metric in Simple Linear Regression used to evaluate how well the regression line explains the variability in the dependent variable y.\n",
        "      Interpretation of R¬≤:   R¬≤ = 1‚àíSSres/SStot\n",
        "\n",
        "     where:  \n",
        "        SSres:Residual Sum of Squares (unexplained variance)\n",
        "\n",
        "        SStot: Total Sum of Squares (total variance in the data)\n",
        "\n",
        "#8.What is Multiple Linear Regression?\n",
        "  - Multiple Linear Regression (MLR) is an extension of Simple Linear Regression that models the relationship between a dependent variable and two or more independent variables using a linear equation.\n",
        "\n",
        "     The general form of a multiple linear regression model is:\n",
        "\n",
        "       y = Œ≤0 + Œ≤1x1 + Œ≤2x2 +....+ Œ≤pxp + Œµ\n",
        "\n",
        "     Where:\n",
        "           y: Dependent (response) variable\n",
        "           x1,x2,‚Ä¶,xp:Independent (predictor) variables\n",
        "           Œ≤0:Intercept\n",
        "           Œ≤1,Œ≤2,‚Ä¶,Œ≤p: Coefficients for each predictor\n",
        "           Œµ: Error term\n",
        "\n",
        "#9.What is the main difference between Simple and Multiple Linear Regression?\n",
        "  - The main difference between Simple and Multiple Linear Regression lies in  the number of independent variables (predictors) used to model the relationship with the dependent variable.\n",
        "\n",
        "#10.What are the key assumptions of Multiple Linear Regression?\n",
        "   - The key assumptions of Multiple Linear Regression (MLR) ensure that the model produces reliable, unbiased, and interpretable results. Violating these assumptions can lead to incorrect conclusions.\n",
        "\n",
        "#11.What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "   - Constant variance (this property is called homoscedasticity)\n",
        "\n",
        "   - Be randomly spread without forming any patterns,\n",
        "     When this assumption is violated (i.e., variance changes with levels of the independent variables), we have heteroscedasticity.\n",
        "     It affect of MLR by inefficient Estimates, incorrect Standard Errors, Poor Prediction Accuracy\n",
        "\n",
        "     Example: In predicting income based on education, residuals might be small for low-income individuals and large for high-income individuals ‚Äî indicating increasing variance.\n",
        "\n",
        "#12.How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "   - To improve a Multiple Linear Regression (MLR) model with high multicollinearity, you need to reduce the strong correlation between the independent variables.To improve MLR model by remove Highly Correlated Predictors, Use Principal Component Analysis, apply Regularization Techniques, combine Variables, Center or Standardize Predictors\n",
        "   \n",
        "#13.What are some common techniques for transforming categorical variables for use in regression models?\n",
        "   - In regression models, categorical variables must be transformed into numerical format since the model cannot handle text or categories directly.\n",
        "   - For transforming categorical variables we are use One-Hot Encoding (Dummy Variables), Label Encoding etc.\n",
        "\n",
        "#14.What is the role of interaction terms in Multiple Linear Regression?\n",
        "   - In Multiple Linear Regression (MLR), interaction terms play a critical role in modeling the combined effect of two or more predictors on the dependent variable.\n",
        "\n",
        "     An interaction term is created by multiplying two (or more) independent variables together. For instance, if you have two predictors, X1 and X2, the interaction term is:Interaction = X1 ‚ãÖ X2\n",
        "\n",
        "     The regression model becomes:Y=Œ≤0+Œ≤1X1+Œ≤2X2+Œ≤3(X1 ‚ãÖX2)+Œµ\n",
        "\n",
        "#15.How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "   - The main difference between simple linear regression and multiple linear regression is the number of independent variables used in the model. In simple linear regression, we use one independent variable, while in multiple linear regression, we use two or more independent variables.\n",
        "\n",
        "#16.What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "   - In regression analysis, the slope represents the rate of change between the independent variable (X) and the dependent variable (Y). It tells us how much Y is expected to change when X increases by one unit, holding other variables constant (in multiple regression).\n",
        "\n",
        "   Impact on Predictions:\n",
        "   - The slope directly influences predictions made by the regression model.\n",
        "   - A positive slope means Y increases as X increases.\n",
        "   - A negative slope means Y decreases as X increases.\n",
        "   - If the slope is zero, X has no effect on Y.\n",
        "\n",
        "#17. How does the intercept in a regression model provide context for the relationship between variables\n",
        "   - In a regression model, the intercept is the predicted value of the dependent variable (Y) when all independent variables (X) are zero.\n",
        "   \n",
        "   Interpretation in Simple Linear Regression (Y = mX + c):\n",
        "   Intercept (c): The value of Y when X = 0.\n",
        "   It provides a baseline or starting point for the relationship between X and Y.\n",
        "   Example:\n",
        "            Model: Y = 4X + 10\n",
        "     When X = 0, Y = 10 ‚Üí So, 10 is the expected value of Y at the origin.\n",
        "     Interpret when X = 0 is within the range of your data.\n",
        "     Ignore when X = 0 has no practical meaning (e.g., negative income, zero house size).\n",
        "\n",
        "#18.What are the limitations of using R¬≤ as a sole measure of model performance?\n",
        "   - The R-squared (R¬≤) value is a commonly used metric in regression analysis to assess the goodness-of-fit of a model. It measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
        "\n",
        "   While R¬≤ can be useful in determining the strength of the relationship between variables, it has several limitations when it comes to drawing conclusions about correlation\n",
        "\n",
        "#19.How would you interpret a large standard error for a regression coefficient?\n",
        "   - A large standard error for a coefficient implies that:\n",
        "   1. The coefficient estimate is unstable\n",
        "     ‚Üí It could vary a lot if you repeated the analysis with different samples.\n",
        "   2. Low precision in estimation\n",
        "     ‚Üí The coefficient might not be significantly different from zero.\n",
        "   3. Weaker statistical evidence\n",
        "     ‚Üí The variable might not be a meaningful predictor\n",
        "\n",
        "#20.How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "   - Plot: Residuals vs. Fitted (Predicted) Values\n",
        "\n",
        "         X-axis: Fitted (predicted) values from your regression model.\n",
        "\n",
        "         Y-axis: Residuals (actual value ‚Äì predicted value).\n",
        "\n",
        "         In a residual vs. fitted plot, heteroscedasticity appears as a changing spread (fan or cone shape) of residuals across fitted values. A good model should show random scatter with no visible patterns.\n",
        "\n",
        "        It is important for Incorrect standard errors,Unreliable hypothesis tests (e.g., t-tests),Inefficient estimates\n",
        "\n",
        "#21.What does it mean if a Multiple Linear Regression model has a high R¬≤ but low adjusted R¬≤?\n",
        "   - A high R¬≤ but low adjusted R¬≤ suggests that your model includes irrelevant variables that inflate R¬≤ artificially without improving model quality. It's a warning sign to simplify or refine your model.\n",
        "\n",
        "#22.Why is it important to scale variables in Multiple Linear Regression?\n",
        "   - Scaling variables in Multiple Linear Regression helps ensure numerical stability, fair comparison of coefficients, and is essential for regularized models. It leads to better interpretation, faster convergence, and more reliable results.\n",
        "\n",
        "#23.What is polynomial regression?\n",
        "   - Polynomial Regression is a type of regression analysis where the relationship between the independent variable (X) and the dependent variable (Y) is modeled as an nth-degree polynomial.\n",
        "     It is an extension of linear regression that allows for curved relationships between variables.\n",
        "\n",
        "#24.How does polynomial regression differ from linear regression?\n",
        "   - Linear regression models a straight-line relationship, while polynomial regression captures non-linear trends using powers of the independent variable.\n",
        "     Polynomial regression offers more flexibility but at the cost of higher risk of overfitting.\n",
        "\n",
        "#25.When is polynomial regression used?\n",
        "   - Polynomial regression is used when data shows a curved trend that cannot be captured by a straight line. It‚Äôs especially useful in science, engineering, and economics for modeling nonlinear but continuous relationships.\n",
        "\n",
        "#26.What is the general equation for polynomial regression?\n",
        "   - Polynomial regression models the relationship between the dependent variable Y and the independent variable X as an nth-degree polynomial:\n",
        "\n",
        "        Y = Œ≤0 + Œ≤1X + Œ≤2x¬≤ + Œ≤3x¬≥  +....+ Œ≤nxn + Œµ\n",
        "\n",
        "        Where: Y: Dependent (response) variable\n",
        "               X: Independent (predictor) variable\n",
        "               Œ≤0: Intercept term\n",
        "               Œ≤1 ,Œ≤2 ,....,Œ≤n: Coefficients of the polynomial terms\n",
        "               x¬≤, x¬≥,....xn: Higher-order powers of X\n",
        "               Œµ: Random error (residual)\n",
        "\n",
        "#27.Can polynomial regression be applied to multiple variables?\n",
        "   - Yes Polynomial Regression Can Be Applied to Multiple Variables.This is   known as Multivariate Polynomial Regression, where the model includes polynomial terms of multiple independent variables, possibly including interactions between them.\n",
        "\n",
        "#28.What are the limitations of polynomial regression?\n",
        "     1.Risk of Overfitting\n",
        "     2.Poor Extrapolation\n",
        "     3.Numerical Instability\n",
        "     4.Loss of Interpretability\n",
        "     5.Curse of Dimensionality\n",
        "\n",
        "#29.What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "       Train-Test Split:Fit the model on a training set, then test it on a held-out test set. If test error increases as degree increases, you're overfitting.\n",
        "       k-Fold Cross-Validation: Divide the dataset into k parts, train on k-1 parts, validate on the remaining part, and repeat k times. More reliable than a single train-test split.Use for Comparing multiple degrees, then choose the one with lowest average validation error.\n",
        "\n",
        "#30.Why is visualization important in polynomial regression?\n",
        "   - Visualization is critically important in polynomial regression because it provides intuitive, immediate insight into how well your model fits the data‚Äîsomething numbers alone often can't reveal.\n",
        "\n",
        "#31.How is polynomial regression implemented in Python?\n",
        "   - Polynomial regression in Python is most commonly implemented using scikit-learn, often with PolynomialFeatures to expand input features and LinearRegression to fit the model.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EyfNOvo2QpQS"
      }
    }
  ]
}